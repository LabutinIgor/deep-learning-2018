{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigment: Neural network basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soft deadline: 16.09.18 at 23.59\n",
    "\n",
    "Hard deadline: 18.09.18 at 23.59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task I intentionally provide no boilerplate code, because very puprpose of this task is getting you comforatable with basic code template for desiging NNs in pytorch. I higly recommend you to revisit all the last seminar materials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task\n",
    "\n",
    "* Implement simple **fully-convolutional** neural architecture for classification. Make sure it is small enought to run on your home machine.\n",
    "* Provide dataset visulization.\n",
    "* Provide train/test split and validation\n",
    "\n",
    "#### Requirements\n",
    "\n",
    "* Architecture should derive from `torch.nn.Module`\n",
    "* Use `torch.utils.data.Dataset` and `torch.utils.data.DataLoader`. But if you manage co simplify this step using dataset `torchivision`, I will only encourage you.\n",
    "* Implement at least one data transformer, but make sure it is useful for classification task.\n",
    "* Use FashionMNIST dataset https://github.com/zalandoresearch/fashion-mnist\n",
    "* Make sure you can fix random seed for all components of your code to make experiments reproducible\n",
    "* Since you architecure should be fully-convolutional, make sure it does not depend on input size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from IPython import display\n",
    "from sklearn import datasets, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method MNIST.__len__ of Dataset FashionMNIST\n",
      "    Number of datapoints: 60000\n",
      "    Split: train\n",
      "    Root Location: ./\n",
      "    Transforms (if any): Compose(\n",
      "                             ToTensor()\n",
      "                             Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "                         )\n",
      "    Target Transforms (if any): None>\n",
      "<bound method MNIST.__len__ of Dataset FashionMNIST\n",
      "    Number of datapoints: 10000\n",
      "    Split: test\n",
      "    Root Location: ./\n",
      "    Transforms (if any): Compose(\n",
      "                             ToTensor()\n",
      "                             Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "                         )\n",
      "    Target Transforms (if any): None>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 4\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        #      or something else...\n",
    "\n",
    "train_set = torchvision.datasets.FashionMNIST(\"./\", train=True, transform=transform, download=True)\n",
    "test_set = torchvision.datasets.FashionMNIST(\"./\", train=False, transform=transform, download=True)\n",
    "print(train_set.__len__)\n",
    "print(test_set.__len__)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE)\n",
    "# fst_batch = next(iter(train_loader))\n",
    "\n",
    "# print(fst_batch[0].size(), fst_batch[1].size())\n",
    "# print(fst_batch[0][0])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=686, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n",
      "Loss 2.318204879760742\n",
      "Loss 2.2634143829345703\n",
      "Loss 2.248753070831299\n",
      "Loss 2.2927303314208984\n",
      "Loss 2.3740806579589844\n",
      "Loss 1.7790218591690063\n",
      "Loss 2.1941823959350586\n",
      "Loss 1.5688652992248535\n",
      "Loss 1.810684084892273\n",
      "Loss 1.511436939239502\n",
      "Loss 1.4796191453933716\n",
      "Loss 1.268081784248352\n",
      "Loss 0.7853357195854187\n",
      "Loss 1.6493446826934814\n",
      "Loss 0.9863147735595703\n",
      "Loss 1.175879716873169\n",
      "Loss 1.117272138595581\n",
      "Loss 0.8955055475234985\n",
      "Loss 2.2493650913238525\n",
      "Loss 2.6554465293884277\n",
      "Loss 0.3419641852378845\n",
      "Loss 0.21923577785491943\n",
      "Loss 1.1609759330749512\n",
      "Loss 2.7905001640319824\n",
      "Loss 2.16614031791687\n",
      "Loss 0.7399162650108337\n",
      "Loss 1.1463346481323242\n",
      "Loss 0.8386775255203247\n",
      "Loss 0.4293285012245178\n",
      "Loss 1.2558139562606812\n",
      "Loss 1.5046799182891846\n",
      "Loss 1.6092921495437622\n",
      "Loss 0.7545484304428101\n",
      "Loss 1.2552075386047363\n",
      "Loss 1.9365195035934448\n",
      "Loss 1.240147590637207\n",
      "Loss 1.016963243484497\n",
      "Loss 0.6190913915634155\n",
      "Loss 0.6047664880752563\n",
      "Loss 0.6171815395355225\n",
      "Loss 1.6512103080749512\n",
      "Loss 1.0828373432159424\n",
      "Loss 0.6943308711051941\n",
      "Loss 1.648459792137146\n",
      "Loss 0.7388660907745361\n",
      "Loss 2.0798497200012207\n",
      "Loss 1.8032313585281372\n",
      "Loss 1.6273505687713623\n",
      "Loss 0.1460179090499878\n",
      "Loss 0.9582234621047974\n",
      "Loss 1.1539714336395264\n",
      "Loss 1.7018718719482422\n",
      "Loss 0.7657561898231506\n",
      "Loss 0.13489413261413574\n",
      "Loss 0.7007092237472534\n",
      "Loss 0.9535238742828369\n",
      "Loss 0.8529968857765198\n",
      "Loss 1.308777928352356\n",
      "Loss 1.0449097156524658\n",
      "Loss 1.1155531406402588\n",
      "Loss 0.5413885116577148\n",
      "Loss 0.4419415593147278\n",
      "Loss 1.4993549585342407\n",
      "Loss 0.25505566596984863\n",
      "Loss 0.7230241894721985\n",
      "Loss 1.454153060913086\n",
      "Loss 0.6259012222290039\n",
      "Loss 0.649870753288269\n",
      "Loss 0.76478111743927\n",
      "Loss 1.2280547618865967\n",
      "Loss 1.5068098306655884\n",
      "Loss 1.4051425457000732\n",
      "Loss 0.3328094482421875\n",
      "Loss 0.6217364072799683\n",
      "Loss 1.2406284809112549\n",
      "Loss 1.2711750268936157\n",
      "Loss 0.8828052282333374\n",
      "Loss 0.7708474397659302\n",
      "Loss 0.054099202156066895\n",
      "Loss 0.7636829614639282\n",
      "Loss 0.9355742931365967\n",
      "Loss 0.015288114547729492\n",
      "Loss 0.5271929502487183\n",
      "Loss 1.2781219482421875\n",
      "Loss 0.7318456172943115\n",
      "Loss 0.618384599685669\n",
      "Loss 1.2354586124420166\n",
      "Loss 1.2896755933761597\n",
      "Loss 0.43162989616394043\n",
      "Loss 0.6381019353866577\n",
      "Loss 0.2031940221786499\n",
      "Loss 1.862701177597046\n",
      "Loss 0.3504612445831299\n",
      "Loss 1.2653239965438843\n",
      "Loss 0.3193313479423523\n",
      "Loss 1.4632697105407715\n",
      "Loss 0.5646687746047974\n",
      "Loss 0.246260404586792\n",
      "Loss 0.20354270935058594\n",
      "Loss 1.9191110134124756\n",
      "Loss 1.4750721454620361\n",
      "Loss 0.6197276711463928\n",
      "Loss 0.7247920632362366\n",
      "Loss 0.36553072929382324\n",
      "Loss 0.7047830820083618\n",
      "Loss 0.7893894910812378\n",
      "Loss 1.785597801208496\n",
      "Loss 0.4232807755470276\n",
      "Loss 1.1794549226760864\n",
      "Loss 0.609704852104187\n",
      "Loss 0.043822646141052246\n",
      "Loss 1.1835031509399414\n",
      "Loss 0.7059544324874878\n",
      "Loss 0.5092948079109192\n",
      "Loss 0.4451996088027954\n",
      "Loss 0.9495372772216797\n",
      "Loss 1.1767187118530273\n",
      "Loss 1.2045705318450928\n",
      "Loss 0.6221032738685608\n",
      "Loss 0.7355984449386597\n",
      "Loss 0.030484914779663086\n",
      "Loss 0.4198286533355713\n",
      "Loss 2.2066314220428467\n",
      "Loss 1.8673940896987915\n",
      "Loss 1.291028380393982\n",
      "Loss 0.6059833765029907\n",
      "Loss 0.6056971549987793\n",
      "Loss 1.3976631164550781\n",
      "Loss 1.5536705255508423\n",
      "Loss 1.8823916912078857\n",
      "Loss 0.5843552350997925\n",
      "Loss 2.118342399597168\n",
      "Loss 1.325561761856079\n",
      "Loss 0.878592848777771\n",
      "Loss 0.512575626373291\n",
      "Loss 0.8143351078033447\n",
      "Loss 0.7040243148803711\n",
      "Loss 2.0406455993652344\n",
      "Loss 1.1592217683792114\n",
      "Loss 0.5906045436859131\n",
      "Loss 0.597892165184021\n",
      "Loss 0.9486676454544067\n",
      "Loss 0.65690016746521\n",
      "Loss 0.6735097169876099\n",
      "Loss 1.56916344165802\n",
      "Loss 0.6426874399185181\n",
      "Loss 0.5066111087799072\n",
      "Loss 1.1323795318603516\n",
      "Loss 1.2320924997329712\n",
      "Loss 0.07911062240600586\n",
      "Loss 1.1222443580627441\n",
      "Loss 0.23618066310882568\n",
      "Loss 0.6551718711853027\n",
      "Loss 1.570149302482605\n",
      "Loss 1.317941665649414\n",
      "Loss 0.6042230129241943\n",
      "Loss 0.22450649738311768\n",
      "Loss 1.2924959659576416\n",
      "Loss 1.1613316535949707\n",
      "Loss 0.10385167598724365\n",
      "Loss 1.2483720779418945\n",
      "Loss 1.0606940984725952\n",
      "Loss 0.8346315026283264\n",
      "Loss 0.3453786373138428\n",
      "Loss 0.192876935005188\n",
      "Loss 0.9296281933784485\n",
      "Loss 0.07623815536499023\n",
      "Loss 0.62230384349823\n",
      "Loss 1.1137715578079224\n",
      "Loss 1.2755612134933472\n",
      "Loss 0.15220147371292114\n",
      "Loss 0.009621143341064453\n",
      "Loss 0.6049689054489136\n",
      "Loss 0.824394702911377\n",
      "Loss 1.3003935813903809\n",
      "Loss 0.20117640495300293\n",
      "Loss 0.7281533479690552\n",
      "Loss 0.6773397922515869\n",
      "Loss 0.36378729343414307\n",
      "Loss 1.3835852146148682\n",
      "Loss 0.1996774673461914\n",
      "Loss 1.0153470039367676\n",
      "Loss 0.6372101306915283\n",
      "Loss 0.9302690029144287\n",
      "Loss 0.9986437559127808\n",
      "Loss 0.5960289239883423\n",
      "Loss 0.04644429683685303\n",
      "Loss 0.2478863000869751\n",
      "Loss 0.7627667188644409\n",
      "Loss 0.016213297843933105\n",
      "Loss 2.9430642127990723\n",
      "Loss 0.1389988660812378\n",
      "Loss 0.08457934856414795\n",
      "Loss 0.6132919788360596\n",
      "Loss 0.09733223915100098\n",
      "Loss 1.080805778503418\n",
      "Loss 0.6761002540588379\n",
      "Loss 0.6710559129714966\n",
      "Loss 0.011055231094360352\n",
      "Loss 0.10497546195983887\n",
      "Loss 0.03077554702758789\n",
      "Loss 0.19822204113006592\n",
      "Loss 0.5612497329711914\n",
      "Loss 0.2041376829147339\n",
      "Loss 0.6483244895935059\n",
      "Loss 0.3737053871154785\n",
      "Loss 0.10113978385925293\n",
      "Loss 0.26491427421569824\n",
      "Loss 0.3223226070404053\n",
      "Loss 0.3325071334838867\n",
      "Loss 0.45380353927612305\n",
      "Loss 0.055434584617614746\n",
      "Loss 0.6132078766822815\n",
      "Loss 0.12918150424957275\n",
      "Loss 0.2984985113143921\n",
      "Loss 0.5225039720535278\n",
      "Loss 0.07450985908508301\n",
      "Loss 0.12714314460754395\n",
      "Loss 0.023340582847595215\n",
      "Loss 0.6304774880409241\n",
      "Loss 0.8944830894470215\n",
      "Loss 0.04244685173034668\n",
      "Loss 0.10077667236328125\n",
      "Loss 0.030315518379211426\n",
      "Loss 0.9519901275634766\n",
      "Loss 0.15194594860076904\n",
      "Loss 0.14746594429016113\n",
      "Loss 0.18326008319854736\n",
      "Loss 0.033125877380371094\n",
      "Loss 0.28604936599731445\n",
      "Loss 0.22357892990112305\n",
      "Loss 0.003374338150024414\n",
      "Loss 0.3854334354400635\n",
      "Loss 0.8663429617881775\n",
      "Loss 0.6431273221969604\n",
      "Loss 0.03429675102233887\n",
      "Loss 1.031316876411438\n",
      "Loss 0.1834571361541748\n",
      "Loss 0.3914405107498169\n",
      "Loss 0.12310671806335449\n",
      "Loss 0.14997482299804688\n",
      "Loss 0.5286939144134521\n",
      "Loss 0.2800253629684448\n",
      "Loss 0.01915287971496582\n",
      "Loss 0.39289283752441406\n",
      "Loss 0.6548742651939392\n",
      "Loss 0.19653141498565674\n",
      "Loss 0.08839106559753418\n",
      "Loss 0.08493733406066895\n",
      "Loss 0.41910457611083984\n",
      "Loss 0.35648858547210693\n",
      "Loss 0.3704942464828491\n",
      "Loss 0.4769641160964966\n",
      "Loss 0.19543743133544922\n",
      "Loss 0.09401237964630127\n",
      "Loss 0.22552108764648438\n",
      "Loss 0.08547699451446533\n",
      "Loss 0.24477994441986084\n",
      "Loss 0.008861303329467773\n",
      "Loss 0.02862381935119629\n",
      "Loss 0.0036847591400146484\n",
      "Loss 0.02209651470184326\n",
      "Loss 0.07166433334350586\n",
      "Loss 0.2941240072250366\n",
      "Loss 0.43781042098999023\n",
      "Loss 0.25325989723205566\n",
      "Loss 0.45216548442840576\n",
      "Loss 0.0438002347946167\n",
      "Loss 0.546277642250061\n",
      "Loss 0.07751548290252686\n",
      "Loss 0.013540267944335938\n",
      "Loss 0.3456461429595947\n",
      "Loss 0.8232446908950806\n",
      "Loss 0.27139198780059814\n",
      "Loss 0.12427735328674316\n",
      "Loss 0.026920318603515625\n",
      "Loss 0.018159985542297363\n",
      "Loss 0.10763788223266602\n",
      "Loss 0.09009754657745361\n",
      "Loss 1.535534381866455\n",
      "Loss 0.0020210742950439453\n",
      "Loss 0.7838907241821289\n",
      "Loss 0.5928725600242615\n",
      "Loss 0.48413848876953125\n",
      "Loss 0.2532007694244385\n",
      "Loss 0.16482579708099365\n",
      "Loss 0.0816652774810791\n",
      "Loss 1.0069618225097656\n",
      "Loss 0.004590034484863281\n",
      "Loss 0.03312110900878906\n",
      "Loss 0.012445449829101562\n",
      "Loss 0.2910575866699219\n",
      "Loss 0.005944967269897461\n",
      "Loss 0.07305121421813965\n",
      "Loss 1.0861455202102661\n",
      "Loss 0.053450584411621094\n",
      "Loss 0.3776376247406006\n",
      "Loss 0.48937714099884033\n",
      "Loss 0.6606278419494629\n",
      "Loss 0.07171249389648438\n",
      "Loss 0.5280554294586182\n",
      "Loss 0.14427220821380615\n",
      "Loss 0.12809598445892334\n",
      "Loss 0.928020179271698\n",
      "Loss 0.9442798495292664\n",
      "Loss 0.030788421630859375\n",
      "Loss 0.07265663146972656\n",
      "Loss 0.19691979885101318\n",
      "Loss 0.46547842025756836\n",
      "Loss 0.025263309478759766\n",
      "Loss 0.1619051694869995\n",
      "Loss 0.3362082242965698\n",
      "Loss 0.22578942775726318\n",
      "Loss 0.12892413139343262\n",
      "Loss 0.07656979560852051\n",
      "Loss 0.8739299774169922\n",
      "Loss 0.03467869758605957\n",
      "Loss 0.9084948897361755\n",
      "Loss 1.3404419422149658\n",
      "Loss 1.3178739547729492\n",
      "Loss 0.189355731010437\n",
      "Loss 0.004082918167114258\n",
      "Loss 0.007356762886047363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.1265583038330078\n",
      "Loss 0.8398325443267822\n",
      "Loss 0.17078137397766113\n",
      "Loss 0.17193222045898438\n",
      "Loss 0.14173710346221924\n",
      "Loss 0.2862095832824707\n",
      "Loss 0.01935434341430664\n",
      "Loss 0.12591075897216797\n",
      "Loss 0.3139769434928894\n",
      "Loss 0.938319981098175\n",
      "Loss 0.6501681804656982\n",
      "Loss 0.18384861946105957\n",
      "Loss 0.027099132537841797\n",
      "Loss 0.019523143768310547\n",
      "Loss 0.1345679759979248\n",
      "Loss 0.13850951194763184\n",
      "Loss 0.007781028747558594\n",
      "Loss 2.735799789428711\n",
      "Loss 0.06613945960998535\n",
      "Loss 0.21978628635406494\n",
      "Loss 0.3903985023498535\n",
      "Loss 0.09509682655334473\n",
      "Loss 1.1852006912231445\n",
      "Loss 0.40878140926361084\n",
      "Loss 0.495137095451355\n",
      "Loss 0.01191401481628418\n",
      "Loss 0.028983592987060547\n",
      "Loss 0.017557859420776367\n",
      "Loss 0.2632012367248535\n",
      "Loss 0.48706793785095215\n",
      "Loss 0.20905566215515137\n",
      "Loss 0.40713930130004883\n",
      "Loss 0.2995946407318115\n",
      "Loss 0.14803576469421387\n",
      "Loss 0.09211957454681396\n",
      "Loss 0.3123129606246948\n",
      "Loss 0.35259056091308594\n",
      "Loss 0.31314969062805176\n",
      "Loss 0.052950382232666016\n",
      "Loss 0.5780520439147949\n",
      "Loss 0.06328117847442627\n",
      "Loss 0.20364034175872803\n",
      "Loss 0.29950904846191406\n",
      "Loss 0.024158477783203125\n",
      "Loss 0.07155895233154297\n",
      "Loss 0.026256322860717773\n",
      "Loss 0.6505367159843445\n",
      "Loss 0.5572717189788818\n",
      "Loss 0.023467540740966797\n",
      "Loss 0.1766362190246582\n",
      "Loss 0.007961511611938477\n",
      "Loss 0.8727377653121948\n",
      "Loss 0.038646697998046875\n",
      "Loss 0.09201622009277344\n",
      "Loss 0.1062920093536377\n",
      "Loss 0.03461003303527832\n",
      "Loss 0.41089558601379395\n",
      "Loss 0.13075053691864014\n",
      "Loss 0.0017876625061035156\n",
      "Loss 0.44139528274536133\n",
      "Loss 0.7073394656181335\n",
      "Loss 0.6693000793457031\n",
      "Loss 0.020819425582885742\n",
      "Loss 0.555261492729187\n",
      "Loss 0.1900782585144043\n",
      "Loss 0.2771874666213989\n",
      "Loss 0.16719484329223633\n",
      "Loss 0.09382474422454834\n",
      "Loss 0.2875344753265381\n",
      "Loss 0.12760603427886963\n",
      "Loss 0.008156776428222656\n",
      "Loss 0.23379933834075928\n",
      "Loss 0.5194422006607056\n",
      "Loss 0.10361039638519287\n",
      "Loss 0.060114145278930664\n",
      "Loss 0.06912660598754883\n",
      "Loss 0.5107870101928711\n",
      "Loss 0.3546864986419678\n",
      "Loss 0.3270937204360962\n",
      "Loss 0.36417722702026367\n",
      "Loss 0.1417090892791748\n",
      "Loss 0.12876641750335693\n",
      "Loss 0.12483906745910645\n",
      "Loss 0.0188368558883667\n",
      "Loss 0.24210286140441895\n",
      "Loss 0.0024118423461914062\n",
      "Loss 0.016175031661987305\n",
      "Loss 0.002591371536254883\n",
      "Loss 0.011876821517944336\n",
      "Loss 0.07294487953186035\n",
      "Loss 0.16453099250793457\n",
      "Loss 0.397228479385376\n",
      "Loss 0.1307131052017212\n",
      "Loss 0.3484787940979004\n",
      "Loss 0.024514198303222656\n",
      "Loss 0.3898378610610962\n",
      "Loss 0.055545687675476074\n",
      "Loss 0.01147150993347168\n",
      "Loss 0.280558705329895\n",
      "Loss 0.43905413150787354\n",
      "Loss 0.2128767967224121\n",
      "Loss 0.1267770528793335\n",
      "Loss 0.014488935470581055\n",
      "Loss 0.00991511344909668\n",
      "Loss 0.09250974655151367\n",
      "Loss 0.02042841911315918\n",
      "Loss 1.3881343603134155\n",
      "Loss 0.0005888938903808594\n",
      "Loss 0.4799809455871582\n",
      "Loss 0.30063438415527344\n",
      "Loss 0.3848813772201538\n",
      "Loss 0.16499125957489014\n",
      "Loss 0.08224713802337646\n",
      "Loss 0.05548238754272461\n",
      "Loss 0.5257663726806641\n",
      "Loss 0.003778219223022461\n",
      "Loss 0.032445311546325684\n",
      "Loss 0.004418849945068359\n",
      "Loss 0.2082003355026245\n",
      "Loss 0.005977630615234375\n",
      "Loss 0.08542847633361816\n",
      "Loss 0.7615435123443604\n",
      "Loss 0.008596181869506836\n",
      "Loss 0.2412853240966797\n",
      "Loss 0.3384058475494385\n",
      "Loss 0.729293704032898\n",
      "Loss 0.06200122833251953\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 14, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(14 * 7 * 7, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 14 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(3):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Loss {loss.item()}\")\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 88 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for data in test_loader:\n",
    "    images, labels = data\n",
    "    outputs = net(Variable(images))\n",
    "    _, predicted = torch.max(outputs.data, 1)   # Find the class index with the maximum value.\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
